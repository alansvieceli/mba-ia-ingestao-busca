# Ingest√£o e Busca Sem√¢ntica com LangChain e PostgreSQL (pgvector)

Este projeto implementa um **sistema completo de busca sem√¢ntica baseado em PDF**, utilizando **LangChain**, **PostgreSQL com pgvector** e **LLMs (OpenAI ou Gemini)**.

O sistema permite:

* Ingerir um arquivo PDF
* Armazenar embeddings vetoriais no banco de dados
* Realizar perguntas via CLI
* Obter respostas **exclusivamente com base no conte√∫do do PDF**
* Evitar qualquer tipo de alucina√ß√£o ou uso de conhecimento externo

---

## üìå Funcionalidades

### Ingest√£o

* Leitura de um arquivo PDF local
* Divis√£o do texto em *chunks* de **1000 caracteres com overlap de 150**
* Gera√ß√£o de embeddings
* Persist√™ncia dos vetores no PostgreSQL (pgvector)

### Busca e Resposta

* Interface de linha de comando (CLI)
* Vetoriza√ß√£o da pergunta do usu√°rio
* Busca dos **10 trechos mais relevantes (k=10)** no banco vetorial
* Montagem de prompt restritivo com base **exclusiva** no contexto recuperado
* Gera√ß√£o de resposta via LLM
* Perguntas fora do contexto retornam uma resposta padr√£o

---

## üß† Tecnologias utilizadas

* **Python 3.12+**
* **LangChain**
* **PostgreSQL + pgvector**
* **Docker & Docker Compose**
* **OpenAI ou Google Gemini**

---

## üìÇ Estrutura do projeto

```
‚îú‚îÄ‚îÄ docker-compose.yml
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ ingest.py          # Ingest√£o do PDF
‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Busca sem√¢ntica + montagem do prompt
‚îÇ   ‚îú‚îÄ‚îÄ chat.py            # CLI interativo (end-to-end)
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ p_search.py    # Template de prompt obrigat√≥rio
‚îú‚îÄ‚îÄ document.pdf           # PDF para ingest√£o (padr√£o)
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Pr√©-requisitos

* Python 3.12 ou superior
* Docker e Docker Compose
* Conta na OpenAI **ou** Google Gemini (para gerar API Key)

---

## üêç Ambiente Python

Crie e ative um ambiente virtual:

```bash
python3 -m venv venv
source venv/bin/activate
```

Instale as depend√™ncias:

```bash
pip install -r requirements.txt
```

---

## üêò Banco de dados (PostgreSQL + pgvector)

Suba o banco via Docker:

```bash
docker compose up -d
```

Verifique se est√° rodando:

```bash
docker compose ps
```

---

## üîê Configura√ß√£o do `.env`

Crie o arquivo `.env` a partir do template:

```bash
cp .env.example .env
```

### Exemplo de configura√ß√£o (OpenAI)

```env
# === Provedor ativo ===
ACTIVE_PROVIDER=openai
# valores poss√≠veis: openai | gemini

# === OpenAI ===
OPENAI_API_KEY=COLE_SUA_CHAVE_AQUI
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# === Gemini (opcional) ===
GOOGLE_API_KEY=
GOOGLE_EMBEDDING_MODEL=models/embedding-001

# === Postgres (Docker rodando localmente) ===
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/postgres

# === Nome da cole√ß√£o/tabela vetorial ===
PG_VECTOR_COLLECTION_NAME=documents

# === Caminho do PDF a ser ingerido ===
PDF_PATH=document.pdf
```

---

### üîÑ Alternar entre OpenAI e Gemini

O projeto suporta **apenas um provedor ativo por vez**, controlado pela vari√°vel `ACTIVE_PROVIDER`.

#### Usando OpenAI

```env
ACTIVE_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
```

#### Usando Gemini

```env
ACTIVE_PROVIDER=gemini
GOOGLE_API_KEY=...
GOOGLE_EMBEDDING_MODEL=models/embedding-001
```

> ‚ö†Ô∏è N√£o √© necess√°rio alterar o c√≥digo para trocar o provedor ‚Äî apenas o `.env`.

---

### üìÑ Configura√ß√£o do PDF

O caminho do PDF √© definido pela vari√°vel:

```env
PDF_PATH=document.pdf
```

* Pode ser um caminho relativo (resolvido a partir da raiz do projeto)
* Ou um caminho absoluto:

  ```env
  PDF_PATH=/caminho/completo/para/arquivo.pdf
  ```

---

## üì• Ingest√£o do PDF

Execute o script de ingest√£o:

```bash
python src/ingest.py
```

Esse passo:

* L√™ o PDF configurado em `PDF_PATH`
* Divide o conte√∫do em **chunks de 1000 caracteres com overlap de 150**
* Gera embeddings para cada chunk
* Armazena os vetores no banco PostgreSQL (pgvector)

---

## üîé Busca sem√¢ntica (sem LLM)

> ‚ö†Ô∏è **Aviso**
> Este passo **n√£o √© obrigat√≥rio** para o uso do sistema.
> A busca sem√¢ntica e a montagem do prompt **j√° s√£o executadas automaticamente pelo `chat.py`** no fluxo completo.
>
> Este script existe **apenas para inspe√ß√£o, depura√ß√£o e valida√ß√£o do prompt**, permitindo visualizar exatamente o contexto que ser√° enviado √† LLM, **sem realizar chamadas √† API**.

Para testar apenas a busca e a montagem do prompt (sem chamar a LLM):

```bash
python src/search.py
```

Esse comando:

* solicita uma pergunta no terminal
* busca os **10 trechos mais relevantes** no banco vetorial
* imprime o **prompt completo** que ser√° enviado √† LLM

Esse passo √© √∫til para:

* validar o `CONTEXTO`
* validar o template exigido pelo desafio
* evitar custos desnecess√°rios com LLM

---

## üí¨ Chat via CLI (fluxo completo)

Inicie o chat interativo:

```bash
python src/chat.py
```

Exemplo:

```text
Fa√ßa sua pergunta:
PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milh√µes de reais.
```

### Perguntas fora do contexto

```text
PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: N√£o tenho informa√ß√µes necess√°rias para responder sua pergunta.
```

---

## üìè Regras de resposta

A LLM √© instru√≠da a:

* Responder **somente** com base no contexto recuperado
* N√£o usar conhecimento externo
* N√£o gerar opini√µes
* Retornar uma mensagem padr√£o caso a resposta n√£o esteja explicitamente no PDF

---

## üö® Observa√ß√µes importantes

* O arquivo `.env` **n√£o deve ser commitado**
* Nunca compartilhe suas API Keys
* O custo de uso das APIs √© baixo para PDFs pequenos
* PDFs escaneados (imagem) podem n√£o conter texto extra√≠vel

---

## ‚úÖ Status do projeto

* [x] Estrutura definida
* [x] Banco com pgvector via Docker
* [x] Suporte a OpenAI e Gemini via `ACTIVE_PROVIDER`
* [x] Implementa√ß√£o da ingest√£o
* [x] Implementa√ß√£o da busca sem√¢ntica
* [x] Implementa√ß√£o do chat CLI (end-to-end)

---

## üìå Conclus√£o

Este projeto implementa um fluxo completo de **RAG (Retrieval-Augmented Generation)** de forma expl√≠cita e audit√°vel, atendendo rigorosamente aos requisitos do desafio:

* ingest√£o controlada
* armazenamento vetorial
* busca top-k
* prompt restritivo
* aus√™ncia total de alucina√ß√µes
