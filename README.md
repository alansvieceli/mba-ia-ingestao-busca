# IngestÃ£o e Busca SemÃ¢ntica com LangChain e PostgreSQL (pgvector)

Este projeto implementa um **sistema de busca semÃ¢ntica baseado em PDF**, utilizando **LangChain**, **PostgreSQL com pgvector** e **LLMs (OpenAI ou Gemini)**.

O sistema permite:

* Ingerir um arquivo PDF
* Armazenar embeddings vetoriais no banco de dados
* Realizar perguntas via CLI
* Obter respostas **exclusivamente com base no conteÃºdo do PDF**
* Evitar qualquer tipo de alucinaÃ§Ã£o ou conhecimento externo

---

## ğŸ“Œ Funcionalidades

### IngestÃ£o

* Leitura de um arquivo PDF local
* DivisÃ£o do texto em *chunks* de 1000 caracteres com overlap de 150
* GeraÃ§Ã£o de embeddings
* PersistÃªncia dos vetores no PostgreSQL (pgvector)

### Busca e Resposta

* Interface de linha de comando (CLI)
* VetorizaÃ§Ã£o da pergunta do usuÃ¡rio
* Busca dos 10 trechos mais relevantes no banco vetorial
* GeraÃ§Ã£o de resposta via LLM **somente com base no contexto recuperado**
* Perguntas fora do contexto retornam uma resposta padrÃ£o

---

## ğŸ§  Tecnologias utilizadas

* **Python 3.12+**
* **LangChain**
* **PostgreSQL + pgvector**
* **Docker & Docker Compose**
* **OpenAI ou Google Gemini**

---

## ğŸ“‚ Estrutura do projeto

```
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingest.py      # IngestÃ£o do PDF
â”‚   â”œâ”€â”€ search.py      # Busca semÃ¢ntica
â”‚   â”œâ”€â”€ chat.py        # CLI interativo
â”œâ”€â”€ document.pdf       # PDF para ingestÃ£o
â””â”€â”€ README.md
```

---

## âš™ï¸ PrÃ©-requisitos

* Python 3.12 ou superior
* Docker e Docker Compose
* Conta na OpenAI **ou** Google Gemini (para gerar API Key)

---

## ğŸ Ambiente Python

Crie e ative um ambiente virtual:

```bash
python3 -m venv venv
source venv/bin/activate
```

Instale as dependÃªncias:

```bash
pip install -r requirements.txt
```

---

## ğŸ˜ Banco de dados (PostgreSQL + pgvector)

Suba o banco via Docker:

```bash
docker compose up -d
```

Verifique se estÃ¡ rodando:

```bash
docker compose ps
```

---

## ğŸ” ConfiguraÃ§Ã£o do `.env`

Crie o arquivo `.env` a partir do template:

```bash
cp .env.example .env
```

Exemplo de configuraÃ§Ã£o:

```env
# === Qual provedor estÃ¡ ativo agora ===
ACTIVE_PROVIDER=openai
# valores possÃ­veis: openai | gemini

# === OpenAI ou Gemini ===
API_KEY=COLE_SUA_CHAVE_AQUI
EMBEDDING_MODEL=text-embedding-3-small

# === Postgres (Docker rodando na sua mÃ¡quina) ===
DATABASE_URL=postgresql+psycopg://postgres:postgres@localhost:5432/postgres

# === Nome da coleÃ§Ã£o/tabela vetorial no pgvector ===
PG_VECTOR_COLLECTION_NAME=documents
```

### ğŸ”„ Trocar entre OpenAI e Gemini

Para usar Gemini, basta alterar:

```env
ACTIVE_PROVIDER=gemini
API_KEY=COLE_SUA_CHAVE_DO_GEMINI
EMBEDDING_MODEL=models/embedding-001
```

Nenhuma alteraÃ§Ã£o de cÃ³digo Ã© necessÃ¡ria.

---

## ğŸ“¥ IngestÃ£o do PDF

Execute o script de ingestÃ£o:

```bash
python src/ingest.py
```

Esse passo:

* LÃª o `document.pdf`
* Gera embeddings
* Armazena os vetores no banco

---

## ğŸ’¬ Chat via CLI

Inicie o chat interativo:

```bash
python src/chat.py
```

Exemplo:

```
FaÃ§a sua pergunta:
PERGUNTA: Qual o faturamento da Empresa SuperTechIABrazil?
RESPOSTA: O faturamento foi de 10 milhÃµes de reais.
```

### Perguntas fora do contexto

```
PERGUNTA: Quantos clientes temos em 2024?
RESPOSTA: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.
```

---

## ğŸ“ Regras de resposta

A LLM Ã© instruÃ­da a:

* Responder **somente** com base no contexto recuperado
* NÃ£o usar conhecimento externo
* NÃ£o gerar opiniÃµes
* Retornar uma mensagem padrÃ£o caso a resposta nÃ£o esteja no PDF

---

## ğŸš¨ ObservaÃ§Ãµes importantes

* O arquivo `.env` **nÃ£o deve ser commitado**
* O custo de uso das APIs Ã© baixo para PDFs pequenos
* PDFs escaneados (imagem) podem nÃ£o gerar texto utilizÃ¡vel

---

## âœ… Status do projeto

* [x] Estrutura definida
* [x] Banco com pgvector via Docker
* [x] Suporte a OpenAI e Gemini
* [ ] ImplementaÃ§Ã£o da ingestÃ£o
* [ ] ImplementaÃ§Ã£o da busca
* [ ] ImplementaÃ§Ã£o do chat CLI
